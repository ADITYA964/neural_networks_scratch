{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a binary classification test\n",
      "net architecture :\n",
      " -> Linear(10 , 20) -> relu -> Linear(20 , 1) -> sigmoid\n",
      "Loss at  0   0.8443498254598792\n",
      "Loss at  1   0.8007824106833336\n",
      "Loss at  2   0.7616293803308702\n",
      "Loss at  3   0.7263704691881232\n",
      "Loss at  4   0.6945537818028162\n",
      "Loss at  5   0.6657717063958598\n",
      "Loss at  6   0.6396719071207664\n",
      "Loss at  7   0.6159434635260378\n",
      "Loss at  8   0.5943272920640527\n",
      "Loss at  9   0.5745820768094588\n",
      "Loss at  10   0.5564909592893742\n",
      "Loss at  11   0.5398779670903624\n",
      "Loss at  12   0.5245878385326871\n",
      "Loss at  13   0.510487020656969\n",
      "Loss at  14   0.4974456497601708\n",
      "Loss at  15   0.4853557926559537\n",
      "Loss at  16   0.4741235316466697\n",
      "Loss at  17   0.4636652602216684\n",
      "Loss at  18   0.4539094733340249\n",
      "Loss at  19   0.44479088508054754\n",
      "Loss at  20   0.436253283065999\n",
      "Loss at  21   0.42823957884316866\n",
      "Loss at  22   0.4207076220147381\n",
      "Loss at  23   0.4136154631721044\n",
      "Loss at  24   0.4069252260088147\n",
      "Loss at  25   0.40060680325322257\n",
      "Loss at  26   0.3946298616656293\n",
      "Loss at  27   0.38896913567216557\n",
      "Loss at  28   0.38359986189932627\n",
      "Loss at  29   0.3785009839492775\n",
      "Loss at  30   0.3736535130994403\n",
      "Loss at  31   0.3690389973951033\n",
      "Loss at  32   0.36464012830075315\n",
      "Loss at  33   0.3604427641564209\n",
      "Loss at  34   0.35643318098287174\n",
      "Loss at  35   0.35260096930157\n",
      "Loss at  36   0.3489353059503951\n",
      "Loss at  37   0.3454254195990574\n",
      "Loss at  38   0.3420638181058078\n",
      "Loss at  39   0.3388411650921308\n",
      "Loss at  40   0.33574713533979056\n",
      "Loss at  41   0.33277536818461295\n",
      "Loss at  42   0.32991915319741155\n",
      "Loss at  43   0.3271712041377702\n",
      "Loss at  44   0.3245251194874571\n",
      "Loss at  45   0.3219770078860961\n",
      "Loss at  46   0.31952169861529117\n",
      "Loss at  47   0.3171554728650718\n",
      "Loss at  48   0.31487414415635384\n",
      "Loss at  49   0.31267153011896937\n",
      "Loss at  50   0.3105451166292511\n",
      "Loss at  51   0.3084918053848631\n",
      "Loss at  52   0.30650742320540947\n",
      "Loss at  53   0.3045885149729205\n",
      "Loss at  54   0.3027312261795303\n",
      "Loss at  55   0.3009331775397892\n",
      "Loss at  56   0.29919131015762146\n",
      "Loss at  57   0.2975038566127636\n",
      "Loss at  58   0.29586960241627697\n",
      "Loss at  59   0.29428660187095\n",
      "Loss at  60   0.29275154461021163\n",
      "Loss at  61   0.2912617359067997\n",
      "Loss at  62   0.2898163494886399\n",
      "Loss at  63   0.28841315415031815\n",
      "Loss at  64   0.2870505302934898\n",
      "Loss at  65   0.285727140363097\n",
      "Loss at  66   0.28443985938631894\n",
      "Loss at  67   0.2831876942433775\n",
      "Loss at  68   0.28196996100489724\n",
      "Loss at  69   0.28078507870761593\n",
      "Loss at  70   0.27963185967552134\n",
      "Loss at  71   0.27850902835135716\n",
      "Loss at  72   0.2774155423716548\n",
      "Loss at  73   0.2763502294037655\n",
      "Loss at  74   0.2753131464222327\n",
      "Loss at  75   0.2743034211380408\n",
      "Loss at  76   0.2733192300236751\n",
      "Loss at  77   0.27235978118726123\n",
      "Loss at  78   0.27142459153445614\n",
      "Loss at  79   0.2705129330660806\n",
      "Loss at  80   0.26962412678058123\n",
      "Loss at  81   0.26875694977033904\n",
      "Loss at  82   0.26791084165508683\n",
      "Loss at  83   0.2670845640769679\n",
      "Loss at  84   0.26627778770466226\n",
      "Loss at  85   0.2654907234725735\n",
      "Loss at  86   0.26472221563411\n",
      "Loss at  87   0.2639714192324823\n",
      "Loss at  88   0.26323788861644565\n",
      "Loss at  89   0.2625208662979398\n",
      "Loss at  90   0.26182024702115586\n",
      "Loss at  91   0.2611356704504537\n",
      "Loss at  92   0.2604661958947488\n",
      "Loss at  93   0.2598112776465302\n",
      "Loss at  94   0.25917040822182896\n",
      "Loss at  95   0.25854274562945173\n",
      "Loss at  96   0.25792840450530874\n",
      "Loss at  97   0.2573271765878013\n",
      "Loss at  98   0.25673840487357336\n",
      "Loss at  99   0.25616200091822067\n",
      "Loss at  100   0.25559748036912255\n",
      "Loss at  101   0.25504463545899153\n",
      "Loss at  102   0.2545031633532639\n",
      "Loss at  103   0.2539726989146287\n",
      "Loss at  104   0.2534529730807904\n",
      "Loss at  105   0.2529435117559986\n",
      "Loss at  106   0.25244383943827775\n",
      "Loss at  107   0.2519540066635737\n",
      "Loss at  108   0.25147407977338476\n",
      "Loss at  109   0.2510031152072597\n",
      "Loss at  110   0.25054096754124\n",
      "Loss at  111   0.25008756914286184\n",
      "Loss at  112   0.24964268232837983\n",
      "Loss at  113   0.24920580149925736\n",
      "Loss at  114   0.24877710374907935\n",
      "Loss at  115   0.248356115537725\n",
      "Loss at  116   0.24794269645194394\n",
      "Loss at  117   0.24753673542089896\n",
      "Loss at  118   0.24713795573447564\n",
      "Loss at  119   0.24674634166365647\n",
      "Loss at  120   0.24636170113740036\n",
      "Loss at  121   0.24598379038911014\n",
      "Loss at  122   0.2456122625237589\n",
      "Loss at  123   0.24524678110294149\n",
      "Loss at  124   0.24488733670234275\n",
      "Loss at  125   0.24453403958790068\n",
      "Loss at  126   0.24418658236552596\n",
      "Loss at  127   0.24384480399494463\n",
      "Loss at  128   0.2435085347041429\n",
      "Loss at  129   0.24317758980006252\n",
      "Loss at  130   0.24285172535398056\n",
      "Loss at  131   0.24253099136164824\n",
      "Loss at  132   0.24221544971781128\n",
      "Loss at  133   0.24190478566611917\n",
      "Loss at  134   0.241598805353457\n",
      "Loss at  135   0.2412975176177156\n",
      "Loss at  136   0.24100117451888803\n",
      "Loss at  137   0.24070941672816226\n",
      "Loss at  138   0.2404220603933773\n",
      "Loss at  139   0.24013902962228353\n",
      "Loss at  140   0.23986017620990865\n",
      "Loss at  141   0.2395855748259548\n",
      "Loss at  142   0.23931497086756398\n",
      "Loss at  143   0.2390481789010874\n",
      "Loss at  144   0.23878539189326758\n",
      "Loss at  145   0.23852651880863293\n",
      "Loss at  146   0.23827172657393456\n",
      "Loss at  147   0.2380207241882443\n",
      "Loss at  148   0.2377733997296079\n",
      "Loss at  149   0.23752968562191049\n",
      "Loss at  150   0.23728952465691483\n",
      "Loss at  151   0.23705298007627695\n",
      "Loss at  152   0.23682023530677823\n",
      "Loss at  153   0.23659084884535117\n",
      "Loss at  154   0.2363647358654597\n",
      "Loss at  155   0.23614167947047854\n",
      "Loss at  156   0.23592158605596192\n",
      "Loss at  157   0.23570433739156832\n",
      "Loss at  158   0.23548985929605987\n",
      "Loss at  159   0.23527840928676647\n",
      "Loss at  160   0.23506963404488695\n",
      "Loss at  161   0.2348634838578558\n",
      "Loss at  162   0.234660026479804\n",
      "Loss at  163   0.23445892940746083\n",
      "Loss at  164   0.23426025049614496\n",
      "Loss at  165   0.2340639585890909\n",
      "Loss at  166   0.23386995580907738\n",
      "Loss at  167   0.23367787564009912\n",
      "Loss at  168   0.23348795916057338\n",
      "Loss at  169   0.23330056641769453\n",
      "Loss at  170   0.23311559426918843\n",
      "Loss at  171   0.23293291316763015\n",
      "Loss at  172   0.23275235496336\n",
      "Loss at  173   0.2325740075977955\n",
      "Loss at  174   0.23239763934061777\n",
      "Loss at  175   0.23222329086589663\n",
      "Loss at  176   0.2320509814987587\n",
      "Loss at  177   0.23188047353601002\n",
      "Loss at  178   0.23171171230513096\n",
      "Loss at  179   0.2315448195632264\n",
      "Loss at  180   0.23137984620630708\n",
      "Loss at  181   0.23121665061219898\n",
      "Loss at  182   0.23105514722146567\n",
      "Loss at  183   0.23089545323674185\n",
      "Loss at  184   0.2307375360586744\n",
      "Loss at  185   0.23058109821120684\n",
      "Loss at  186   0.23042639964771244\n",
      "Loss at  187   0.23027335126646153\n",
      "Loss at  188   0.23012188862074195\n",
      "Loss at  189   0.22997193970293447\n",
      "Loss at  190   0.22982367589079414\n",
      "Loss at  191   0.2296769687172104\n",
      "Loss at  192   0.22953174223101344\n",
      "Loss at  193   0.2293879766738663\n",
      "Loss at  194   0.2292457674029933\n",
      "Loss at  195   0.22910515273902904\n",
      "Loss at  196   0.2289659801310205\n",
      "Loss at  197   0.22882820285031538\n",
      "Loss at  198   0.2286918739291632\n",
      "Loss at  199   0.22855674106501667\n",
      "for gradient descenet \n",
      " accuracy =  91.22666666666667\n",
      "Loss at  1   0.22173955606988105\n",
      "Loss at  2   0.20935077218084075\n",
      "Loss at  3   0.20315636350107016\n",
      "Loss at  4   0.19936239342224096\n",
      "Loss at  5   0.19693706648303946\n",
      "for stochaistic gradient descenet without momentum\n",
      " accuracy =  92.90333333333334\n",
      "Loss at  1   0.21949962016842736\n",
      "Loss at  2   0.20908777471166157\n",
      "Loss at  3   0.20293524216671432\n",
      "Loss at  4   0.19897183146354047\n",
      "Loss at  5   0.1962500885766216\n",
      "for stochaistic gradient descenet with momentum\n",
      " accuracy =  92.91\n",
      "Loss at  1   0.5615028643608198\n",
      "Loss at  2   0.3760327934345861\n",
      "Loss at  3   0.2784827968720779\n",
      "Loss at  4   0.23717816823533655\n",
      "Loss at  5   0.23706810690200358\n",
      "Loss at  6   0.25163330495565844\n",
      "Loss at  7   0.26144892458879937\n",
      "Loss at  8   0.2603589454773324\n",
      "Loss at  9   0.2504539204931681\n",
      "Loss at  10   0.2362249817224883\n",
      "Loss at  11   0.22459356451831627\n",
      "Loss at  12   0.2182005827456216\n",
      "Loss at  13   0.21485755529514264\n",
      "Loss at  14   0.2135342068284259\n",
      "Loss at  15   0.21268105346164595\n",
      "Loss at  16   0.21081346771555998\n",
      "Loss at  17   0.20796750128897387\n",
      "Loss at  18   0.20446057346749724\n",
      "Loss at  19   0.20039723151907995\n",
      "Loss at  20   0.19599188717156704\n",
      "Loss at  21   0.19225122847716383\n",
      "Loss at  22   0.1905325459896042\n",
      "Loss at  23   0.18944470479268302\n",
      "Loss at  24   0.18619071940831608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at  25   0.18264754987567747\n",
      "Loss at  26   0.18085978770807865\n",
      "Loss at  27   0.18018368505444993\n",
      "Loss at  28   0.17951313464465268\n",
      "Loss at  29   0.17877711685373313\n",
      "Loss at  30   0.17718447802829965\n",
      "Loss at  31   0.17521227289651264\n",
      "Loss at  32   0.17432232639801548\n",
      "Loss at  33   0.1736537204883624\n",
      "Loss at  34   0.17232332947940246\n",
      "Loss at  35   0.17110039987710698\n",
      "Loss at  36   0.1701686635428682\n",
      "Loss at  37   0.16909916129453506\n",
      "Loss at  38   0.16790820609292684\n",
      "Loss at  39   0.16652602511556552\n",
      "Loss at  40   0.1650251078718627\n",
      "Loss at  41   0.1638978777847775\n",
      "Loss at  42   0.1624860991643307\n",
      "Loss at  43   0.16117443740699178\n",
      "Loss at  44   0.1601526679822643\n",
      "Loss at  45   0.15904054999667094\n",
      "Loss at  46   0.15768524695221406\n",
      "Loss at  47   0.15627129549978896\n",
      "Loss at  48   0.15500983475780267\n",
      "Loss at  49   0.15359158306328893\n",
      "Loss at  50   0.15196835235085832\n",
      "Loss at  51   0.15033253559306045\n",
      "Loss at  52   0.14877054320537791\n",
      "Loss at  53   0.14718294115360467\n",
      "Loss at  54   0.14546158098885478\n",
      "Loss at  55   0.14372491422743966\n",
      "Loss at  56   0.1419937242286373\n",
      "Loss at  57   0.14015983391024772\n",
      "Loss at  58   0.1383818780001109\n",
      "Loss at  59   0.13667101455515426\n",
      "Loss at  60   0.13495228157401176\n",
      "Loss at  61   0.13314554904039755\n",
      "Loss at  62   0.13149511182027018\n",
      "Loss at  63   0.12982636546504425\n",
      "Loss at  64   0.1281679075955576\n",
      "Loss at  65   0.12665743093661502\n",
      "Loss at  66   0.12524245148993807\n",
      "Loss at  67   0.12381292364533332\n",
      "Loss at  68   0.12253621652717478\n",
      "Loss at  69   0.12142409661910396\n",
      "Loss at  70   0.12039162586056114\n",
      "Loss at  71   0.119398502114543\n",
      "Loss at  72   0.11845397436195212\n",
      "Loss at  73   0.11759871075330948\n",
      "Loss at  74   0.11680022259067173\n",
      "Loss at  75   0.11605561592587368\n",
      "Loss at  76   0.11521722242229135\n",
      "Loss at  77   0.11445390627577129\n",
      "Loss at  78   0.11384407470326083\n",
      "Loss at  79   0.113263504564795\n",
      "Loss at  80   0.11266891631352131\n",
      "for Adam:\n",
      " accuracy =  96.30333333333333\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import nn\n",
    "import numpy as np\n",
    "from optimizer import optimizer\n",
    "\n",
    "def test_run():\n",
    "    \"\"\"\n",
    "    Sample test run.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    # test run for binary classification problem:\n",
    "    np.random.seed(3)\n",
    "    print('Running a binary classification test')\n",
    "\n",
    "    #Generate sample binary classification data\n",
    "    data = datasets.make_classification(n_samples=30000,n_features=10,n_classes=2)\n",
    "    X= data[0].T\n",
    "    Y = (data[1].reshape(30000,1)).T\n",
    "    net = nn.nn([10,20,1],['relu','sigmoid'])\n",
    "    net.cost_function = 'CrossEntropyLoss'\n",
    "    print('net architecture :')\n",
    "    print(net)\n",
    "    #Optimize using standard gradient descenet\n",
    "    optim = optimizer.gradientDescentOptimizer\n",
    "    optim(X,Y,net,alpha=0.07,epoch=200,lamb=0.05,print_at=1)\n",
    "    output = net.forward(X)\n",
    "    #Convert the probabilities to output values\n",
    "    output = 1*(output>=0.5)\n",
    "    accuracy = np.sum(output==Y)/30000\n",
    "    print('for gradient descenet \\n accuracy = ' ,accuracy*100)\n",
    "\n",
    "    #Optimize using SGD without momentum\n",
    "    net = nn.nn([10,20,1],['relu','sigmoid'])\n",
    "    net.cost_function = 'CrossEntropyLoss'\n",
    "    optim = optimizer.SGDOptimizer\n",
    "    optim(X,Y,net,128,alpha=0.07,epoch=5,lamb=0.05,print_at=1)\n",
    "    output = net.forward(X)\n",
    "    output = 1*(output>=0.5)\n",
    "    accuracy = np.sum(output==Y)/30000\n",
    "    print('for stochaistic gradient descenet without momentum\\n accuracy = ' ,accuracy*100)\n",
    "\n",
    "    #optimize using  SGD with momentum\n",
    "    net = nn.nn([10,20,1],['relu','sigmoid'])\n",
    "    net.cost_function = 'CrossEntropyLoss'\n",
    "    optim = optimizer.SGDOptimizer\n",
    "    optim(X,Y,net,128,alpha=0.07,epoch=5,lamb=0.05,print_at=1,momentum=0.9)\n",
    "    output = net.forward(X)\n",
    "    output = 1*(output>=0.5)\n",
    "    accuracy = np.sum(output==Y)/30000\n",
    "    print('for stochaistic gradient descenet with momentum\\n accuracy = ' ,accuracy*100)\n",
    "\n",
    "    #optimize using  ADAM\n",
    "    net = nn.nn([10,20,1],['relu','sigmoid'])\n",
    "    net.cost_function = 'CrossEntropyLoss'\n",
    "    optim = optimizer.AdamOptimizer\n",
    "    optim(X,Y,net,alpha=0.07,epoch=80,lamb=0.05,print_at=1)\n",
    "    output = net.forward(X)\n",
    "    output = 1*(output>=0.5)\n",
    "    accuracy = np.sum(output==Y)/30000\n",
    "    print('for Adam:\\n accuracy = ' ,accuracy*100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn\n",
    "import numpy as np\n",
    "\n",
    "class optimizer:\n",
    "    @staticmethod\n",
    "    def gradientDescentOptimizer(input,mappings,net,alpha=0.001,lamb=0, epoch=100,print_at=5,prnt=True,update=True):\n",
    "        \"\"\"\n",
    "        Performs gradient descent on the given network setting the default value of epoch and alpha if not provided otherwise\n",
    "        :param input  : input for neural net\n",
    "        :param mapping: Correct output of the function\n",
    "        :param net    : nn.nn object which provides the network architecture\n",
    "        :param alpha  : Learning rate\n",
    "        :param lamb   : Regularization parameter\n",
    "        :param epoch  : Number of iterations\n",
    "        :param print_at: Print at multiples of 'print_at'\n",
    "        :param prnt   : Print if prnt=true\n",
    "        \"\"\"\n",
    "        net.lamb = lamb\n",
    "\n",
    "        for i in range(epoch):\n",
    "            net.cache = []\n",
    "            prediction = net.forward(input)\n",
    "            loss_function = (net.cost_function).lower()\n",
    "            loss,regularization_cost = None,0\n",
    "            if loss_function == 'mseloss':\n",
    "                loss = net.MSELoss(prediction,mappings)\n",
    "            if loss_function == 'crossentropyloss':\n",
    "                loss = net.CrossEntropyLoss(prediction,mappings)\n",
    "                \n",
    "            if prnt and i%print_at==0 :\n",
    "                print('Loss at ',i, ' ' ,loss)\n",
    "\n",
    "            net.backward(prediction,mappings)\n",
    "            if update:\n",
    "                net.parameters = optimizer.update_params(net.parameters,net.grads,alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def SGDOptimizer(input,mappings,net,mini_batch_size=64,alpha=0.001,lamb=0,momentum=None,epoch=5,print_at=5,prnt=True):\n",
    "        '''\n",
    "        Performs Stochaitic gradient descent on the given network\n",
    "        -Generates mini batches of given size using random permutation\n",
    "        -Uses gradient descent on each mini batch separately\n",
    "        :param input  : input for neural net\n",
    "        :param mapping: Correct output of the function\n",
    "        :param net    : nn.nn object which provides the network architecture\n",
    "        :param batch_size: Batch size to be used witch SGD\n",
    "        :param alpha  : Learning rate\n",
    "        :param lamb   : Regularization parameter\n",
    "        :param momentum: Momentum Hyper parameter\n",
    "        :param epoch  : Number of iterations\n",
    "        :param print_at: Print at multiples of 'print_at'\n",
    "        :param prnt   : Print if prnt=true\n",
    "        :return : None\n",
    "        '''\n",
    "        batch_size = input.shape[1]\n",
    "        mini_batches = []\n",
    "        \n",
    "        permutation = list(np.random.permutation(batch_size))\n",
    "        shuffled_input = input[:,permutation]\n",
    "        shuffled_mappings = (mappings[:,permutation])\n",
    "\n",
    "        num_complete_batches = int(np.floor(batch_size/mini_batch_size))\n",
    "        \n",
    "        #Separate the complete mini_batches\n",
    "        for i in range(0,num_complete_batches):\n",
    "            mini_batch_input = shuffled_input[:,i*mini_batch_size:(i+1)*mini_batch_size]\n",
    "            mini_batch_mappings = shuffled_mappings[:,i*mini_batch_size:(i+1)*mini_batch_size]\n",
    "            mini_batch = (mini_batch_input,mini_batch_mappings)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        #Separate the incomplete mini batch if any\n",
    "        if batch_size % mini_batch_size != 0:\n",
    "            mini_batch_input = shuffled_input[:,batch_size - num_complete_batches*mini_batch_size : batch_size]\n",
    "            mini_batch_mappings = shuffled_mappings[:,batch_size - num_complete_batches*mini_batch_size : batch_size]\n",
    "            mini_batch = (mini_batch_input,mini_batch_mappings)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        #Initialize momentum velocity\n",
    "        velocity = {}\n",
    "        if momentum != None:\n",
    "            for i in range(int(len(net.parameters)/2)):\n",
    "                velocity['dW'+str(i+1)] = np.zeros(net.parameters['W'+str(i+1)].shape)\n",
    "                velocity['db'+str(i+1)] = np.zeros(net.parameters['b'+str(i+1)].shape)\n",
    "        \n",
    "\n",
    "        for i in range(1,epoch+1):\n",
    "\n",
    "            for batches in range(len(mini_batches)):\n",
    "\n",
    "                if momentum != None:\n",
    "                    optimizer.gradientDescentOptimizer(input,mappings,net,alpha,lamb,epoch=1,prnt=False,update=False)\n",
    "                    for j in range(int(len(net.parameters)/2)):\n",
    "                        velocity['dW' + str(j+1)] = momentum*velocity['dW'+str(j+1)] + (1-momentum)*net.grads['dW'+str(j+1)]\n",
    "                        velocity['db' + str(j+1)] = momentum*velocity['db'+str(j+1)] + (1-momentum)*net.grads['db'+str(j+1)]\n",
    "                    net.parameters = optimizer.update_params(net.parameters,velocity,alpha)\n",
    "                else:\n",
    "                    optimizer.gradientDescentOptimizer(input,mappings,net,alpha,lamb,epoch=1,prnt=False)\n",
    "\n",
    "            prediction = net.forward(input)\n",
    "            loss = None \n",
    "            loss_function = net.cost_function.lower()\n",
    "            if loss_function == 'mseloss':\n",
    "                loss = net.MSELoss(prediction,mappings)\n",
    "            if loss_function == 'crossentropyloss':\n",
    "                loss = net.CrossEntropyLoss(prediction,mappings)\n",
    "            \n",
    "            if i%print_at == 0:\n",
    "                print('Loss at ', i , ' ' , loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def AdamOptimizer(input,mappings,net,alpha=0.001,lamb=0,betas=(0.9,0.99),epoch=5,print_at=5,prnt=True):\n",
    "        '''\n",
    "        Performs Adam otimization on the given network.\n",
    "        :param input  : input for neural net\n",
    "        :param mapping: Correct output of the function\n",
    "        :param net    : nn.nn object which provides the network architecture\n",
    "        :param alpha  : Learning rate\n",
    "        :param lamb   : Regularization parameter\n",
    "        :param betas: Adam Hyper parameters\n",
    "        :param epoch  : Number of iterations\n",
    "        :param print_at: Print at multiples of 'print_at'\n",
    "        :param prnt   : Print if prnt=true\n",
    "        :return : None\n",
    "        '''\n",
    "        batch_size = input.shape[1]\n",
    "\n",
    "        velocity, square = {},{}\n",
    "        for i in range(int(len(net.parameters)/2)):\n",
    "            velocity['dW'+str(i+1)] = np.zeros(net.parameters['W'+str(i+1)].shape)\n",
    "            velocity['db'+str(i+1)] = np.zeros(net.parameters['b'+str(i+1)].shape)\n",
    "            square['dW'+str(i+1)] = np.zeros(net.parameters['W'+str(i+1)].shape)\n",
    "            square['db'+str(i+1)] = np.zeros(net.parameters['b'+str(i+1)].shape)\n",
    "        \n",
    "        for i in range(1,epoch+1):\n",
    "            \n",
    "            optimizer.gradientDescentOptimizer(input,mappings,net,lamb,epoch=1,prnt=False,update=False)\n",
    "\n",
    "            for j in range(int(len(net.parameters)/2)):\n",
    "                velocity['dW'+str(j+1)] = betas[0]*velocity['dW'+str(j+1)] + (1-betas[0])*net.grads['dW'+str(j+1)]\n",
    "                velocity['db'+str(j+1)] = betas[0]*velocity['db'+str(j+1)] + (1-betas[0])*net.grads['db'+str(j+1)]\n",
    "                square['dW'+str(j+1)] = betas[1]*square['dW'+str(j+1)] + (1-betas[1])*np.power(net.grads['dW'+str(j+1)],2)\n",
    "                square['db'+str(j+1)] = betas[1]*square['db'+str(j+1)] + (1-betas[1])*np.power(net.grads['db'+str(j+1)],2)\n",
    "            \n",
    "            update = {}\n",
    "            for j in range(int(len(net.parameters)/2)):\n",
    "                update['dW' + str(j+1)] = velocity['dW'+ str(j+1)]/(np.sqrt(square['dW'+str(j+1)])+1e-10)\n",
    "                update['db' + str(j+1)] = velocity['db'+ str(j+1)]/(np.sqrt(square['db'+str(j+1)])+1e-10)\n",
    "            \n",
    "            net.parameters = optimizer.update_params(net.parameters,update,alpha)\n",
    "\n",
    "            prediction = net.forward(input)\n",
    "            loss = None \n",
    "            loss_function = net.cost_function.lower()\n",
    "            if loss_function == 'mseloss':\n",
    "                loss = net.MSELoss(prediction,mappings)\n",
    "            if loss_function == 'crossentropyloss':\n",
    "                loss = net.CrossEntropyLoss(prediction,mappings)\n",
    "            \n",
    "            if i%print_at == 0:\n",
    "                print('Loss at ', i , ' ' , loss)       \n",
    "\n",
    "    @staticmethod\n",
    "    def update_params(params,updation,learning_rate):\n",
    "        '''\n",
    "        Updates the parameters using gradients and learning rate provided\n",
    "        \n",
    "        :param params   : Parameters of the network\n",
    "        :param updation    : updation valcues calculated using appropriate algorithms\n",
    "        :param learning_rate: Learning rate for the updation of values in params\n",
    "        :return : Updated params \n",
    "        '''\n",
    "        \n",
    "        for i in range(int(len(params)/2)):\n",
    "            params['W' + str(i+1)] = params['W' + str(i+1)] - learning_rate*updation['dW' + str(i+1)]\n",
    "            params['b' + str(i+1)] = params['b' + str(i+1)] - learning_rate*updation['db' + str(i+1)]\n",
    "        \n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from numpy import float64, ndarray\n",
    "from typing import List, Tuple, Union\n",
    "class nn:\n",
    "    def __init__(self, layer_dimensions: List[int] = [], activations: List[str] = []) -> None:\n",
    "        \"\"\"\n",
    "        Initializes networks's weights and other useful variables.\n",
    "        :param layer_dimensions:\n",
    "        :param activations: To store the activation for each layer\n",
    "        -Parameters contains weights of the layer in form {'Wi':[],'bi':[]}\n",
    "        -Cache contains intermediate results as [[A[i-1],Wi,bi],[Zi]], where i\n",
    "         is layer number.\n",
    "        -activations contains the names of activation function used for that layer\n",
    "        -cost_function  contains the name of cost function to be used\n",
    "        -lamb contains the regularization hyper-parameter\n",
    "        -grads contains the gradients calculated during back-prop in form {'dA(i-1)':[],'dWi':[],'dbi':[]}\n",
    "        -layer_type contains the info about the type of layer( fc, conv etc)\n",
    "        \"\"\"\n",
    "        self.parameters = {}\n",
    "        self.cache = []\n",
    "        self.activations = activations\n",
    "        self.cost_function = ''\n",
    "        self.lamb = 0\n",
    "        self.grads = {}\n",
    "        self.layer_type = ['']\n",
    "        self.hyperparam = {}\n",
    "        self.initialize_parameters(layer_dimensions)\n",
    "        self.check_activations()\n",
    " \n",
    "\n",
    "    def initialize_parameters(self, layer_dimensions: List[int]) -> None:\n",
    "        \"\"\"\n",
    "        Xavier initialization of weights of a network described by given layer\n",
    "        dimensions.\n",
    "        :param layer_dimensions: Dimensions to layers of the network\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        num_layers = int(len(self.parameters)/2)\n",
    "\n",
    "        for i in range(1, len(layer_dimensions)):\n",
    "            self.parameters[\"W\" + str(num_layers+i)] = (\n",
    "                np.sqrt(2/layer_dimensions[i - 1])*np.random.randn(layer_dimensions[i],\n",
    "                                layer_dimensions[i - 1])\n",
    "            )\n",
    "            self.parameters[\"b\" + str(i+num_layers)] = np.zeros((layer_dimensions[i], 1))\n",
    "            self.layer_type.append('fc')\n",
    "\n",
    "    def add_fcn(self,dims: List[int],activations: List[str]) -> None:\n",
    "        '''\n",
    "        Add fully connected layers in between the network\n",
    "        :param dims:list describing dimensions of fully connected networks\n",
    "        :param activations: activations of each layer\n",
    "        '''\n",
    "        self.initialize_parameters(dims)\n",
    "        for i in activations:\n",
    "            self.activations.append(i)\n",
    "\n",
    "    def check_activations(self) -> None:\n",
    "        '''\n",
    "        Checks if activations for all layers are present. Adds 'None' if no activations are provided for a particular layer.\n",
    "        \n",
    "        :returns: None\n",
    "        '''\n",
    "        num_layers = int(len(self.parameters)/2)\n",
    "        while len(self.activations) < num_layers :\n",
    "            self.activations.append(None)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def __linear_forward(A_prev, W, b):\n",
    "        \"\"\"\n",
    "        Linear forward to the current layer using previous activations.\n",
    "        :param A_prev: Previous Layer's activation\n",
    "        :param W: Weights for current layer\n",
    "        :param b: Biases for current layer\n",
    "        :return: Linear cache and current calculated layer\n",
    "        \"\"\"\n",
    "        Z = W.dot(A_prev) + b\n",
    "        linear_cache = [A_prev, W, b]\n",
    "        return Z, linear_cache\n",
    "\n",
    "    def __activate(self, Z, n_layer=1):\n",
    "        \"\"\"\n",
    "        Activate the given layer(Z) using the activation function specified by\n",
    "        'type'.\n",
    "        Note: This function treats 1 as starting index!\n",
    "              First layer's index is 1.\n",
    "        :param Z: Layer to activate\n",
    "        :param n_layer: Layer's index\n",
    "        :return: Activated layer and activation cache\n",
    "        \"\"\"\n",
    "        act_cache = [Z]\n",
    "        act = None\n",
    "        if (self.activations[n_layer - 1]) == None:\n",
    "            act = Z\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"relu\":\n",
    "            act = Z * (Z > 0)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"tanh\":\n",
    "            act = np.tanh(Z)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"sigmoid\":\n",
    "            act = 1 / (1 + np.exp(-Z))\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"softmax\":\n",
    "            act = np.exp(Z-np.max(Z))\n",
    "            act = act/(act.sum(axis=0)+1e-10)\n",
    "        \n",
    "\n",
    "        # assert(act!=None)\n",
    "\n",
    "        return act, act_cache\n",
    "\n",
    "    def forward(self, net_input: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        To forward propagate the entire Network.\n",
    "        :param net_input: Contains the input to the Network\n",
    "        :return: Output of the network\n",
    "        \"\"\"\n",
    "        self.cache = [] \n",
    "        A = net_input\n",
    "        for i in range(1, int(len(self.parameters) / 2)):\n",
    "            W = self.parameters[\"W\" + str(i)]\n",
    "            b = self.parameters[\"b\" + str(i)]\n",
    "            Z = linear_cache = None\n",
    "            if self.layer_type[i] == 'fc':\n",
    "                Z, linear_cache = self.__linear_forward(A, W, b)\n",
    "            elif self.layer_type[i] == 'conv':\n",
    "                hyperparam = self.hyperparam[i]\n",
    "                Z , linear_cache = self.conv_forward(A,W,b,hyperparam)\n",
    "\n",
    "                #flatten the output if the next layer is fully connected\n",
    "            A, act_cache = self.__activate(Z, i)\n",
    "            if  self.layer_type[i]=='conv':\n",
    "                if  self.layer_type[i+1] == 'fc':\n",
    "                    A = A.reshape((A.shape[1]*A.shape[2]*A.shape[3],A.shape[0]))\n",
    " \n",
    "\n",
    "            self.cache.append([linear_cache, act_cache])\n",
    "\n",
    "        # For Last Layer\n",
    "        W = self.parameters[\"W\" + str(int(len(self.parameters) / 2))]\n",
    "        b = self.parameters[\"b\" + str(int(len(self.parameters) / 2))]\n",
    "        Z, linear_cache = self.__linear_forward(A, W, b)\n",
    "        if len(self.activations) == len(self.parameters) / 2:\n",
    "            A, act_cache = self.__activate(Z, len(self.activations))\n",
    "            self.cache.append([linear_cache, act_cache])\n",
    "        else:\n",
    "            A = Z\n",
    "            self.cache.append([linear_cache, [None]])\n",
    "        \n",
    "        return A\n",
    "    '''\n",
    "    !!!!Only works for fully connected networks.!!!!!\n",
    "    def forward_upto(self, net_input, layer_num):\n",
    "        \"\"\"\n",
    "        Calculates forward prop upto layer_num.\n",
    "        :param net_input: Contains the input to the Network\n",
    "        :param layer_num: Layer up to which forward prop is to be calculated\n",
    "        :return: Activations of layer layer_num\n",
    "        \"\"\"\n",
    "        if layer_num == int(len(self.parameters) / 2):\n",
    "            return self.forward(net_input)\n",
    "        else:\n",
    "            A = net_input\n",
    "            for i in range(1, layer_num):\n",
    "                W = self.parameters[\"W\" + str(i)]\n",
    "                b = self.parameters[\"b\" + str(i)]\n",
    "                Z, linear_cache = self.__linear_forward(A, W, b)\n",
    "                A, act_cache = self.__activate(Z, i)\n",
    "                self.cache.append([linear_cache, act_cache])\n",
    "            return A\n",
    "    ''' \n",
    "\n",
    "    def MSELoss(self,prediction: ndarray,mappings: ndarray) -> float64:\n",
    "        '''\n",
    "        Calculates the Mean Squared error with regularization cost(if provided) between output of the network and the real\n",
    "        mappings of a function.\n",
    "        Changes cost_function to appropriate value\n",
    "        :param prediction: Output of the neural net\n",
    "        :param mappings: Real outputs of a function\n",
    "        :return: Mean squared error b/w output and mappings\n",
    "        '''\n",
    "\n",
    "        self.cost_function = 'MSELoss'\n",
    "        loss = np.square(prediction-mappings).mean()/2\n",
    "        regularization_cost = 0\n",
    "        if self.lamb != 0:\n",
    "            for params in range(len(self.cache)):  \n",
    "                regularization_cost = regularization_cost + np.sum(np.square(self.parameters['W'+str(params+1)]))\n",
    "        regularization_cost = (self.lamb/(2*prediction.shape[1]))*regularization_cost\n",
    "        \n",
    "        return loss + regularization_cost\n",
    "\n",
    "    def CrossEntropyLoss(self,prediction: ndarray,mappings: ndarray) -> float64:\n",
    "        '''\n",
    "        Calculates the cross entropy loss between output of the network and the real mappings of a function\n",
    "        Changes cost_function to appropriate value\n",
    "        :param prediction: Output of the neural net\n",
    "        :param mappings: Real outputs of a function\n",
    "        :return: Mean squared error b/w output and mappings\n",
    "        '''\n",
    "        epsilon = 1e-8\n",
    "        self.cost_function = 'CrossEntropyLoss'\n",
    "        loss = -(1/prediction.shape[1])*np.sum( mappings*np.log(prediction+epsilon) + (1-mappings)*np.log(1-prediction+epsilon) )\n",
    "        regularization_cost = 0\n",
    "        if self.lamb != 0:\n",
    "            for params in range(len(self.cache)):\n",
    "                regularization_cost = regularization_cost + np.sum(np.square(self.parameters['W'+str(params+1)]))\n",
    "        regularization_cost = (self.lamb/(2*prediction.shape[1]))*regularization_cost\n",
    "\n",
    "        return loss + regularization_cost\n",
    "    \n",
    "    def output_backward(self,prediction: ndarray,mapping: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calculates the derivative of the output layer(dA)\n",
    "        :param prediction: Output of neural net\n",
    "        :param mapping: Correct output of the function\n",
    "        :param cost_type: Type of Cost function used\n",
    "        :return: Derivative of output layer, dA  \n",
    "        '''\n",
    "        dA = None\n",
    "        cost = self.cost_function\n",
    "        if cost.lower() == 'crossentropyloss':\n",
    "            dA =  -(np.divide(mapping, prediction+1e-10) - np.divide(1 - mapping, 1 - prediction+1e-10))\n",
    "        \n",
    "        elif cost.lower() == 'mseloss':   \n",
    "            dA =  (prediction-mapping)\n",
    "        \n",
    "        return dA\n",
    "    \n",
    "    def deactivate(self,dA: ndarray,n_layer: int) -> Union[ndarray, int]:\n",
    "        '''\n",
    "        Calculates the derivate of dA by deactivating the layer\n",
    "        :param dA: Activated derivative of the layer\n",
    "        :n_layer: Layer number to be deactivated\n",
    "        :return: deact=> derivative of activation \n",
    "        '''\n",
    "        act_cache = self.cache[n_layer-1][1]\n",
    "        dZ = act_cache[0]\n",
    "        deact = None\n",
    "        if self.activations[n_layer - 1] == None:\n",
    "            deact = 1\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"relu\":\n",
    "            deact = 1* (dZ>0)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"tanh\":\n",
    "            deact = 1- np.square(dA)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"sigmoid\" or (self.activations[n_layer - 1]).lower()=='softmax':\n",
    "            s = 1/(1+np.exp(-dZ+1e-10))\n",
    "            deact = s*(1-s)\n",
    "\n",
    "        return deact\n",
    "    \n",
    "    def linear_backward(self,dA: ndarray,n_layer: int) -> Tuple[ndarray, ndarray, ndarray]:\n",
    "        '''\n",
    "        Calculates linear backward propragation for layer denoted by n_layer\n",
    "        :param dA: Derivative of cost w.r.t this layer\n",
    "        :param n_layer: layer number\n",
    "        :return : dZ,dW,db,dA_prev\n",
    "        '''\n",
    "        batch_size = dA.shape[1]\n",
    "        current_cache = self.cache[n_layer-1]\n",
    "        linear_cache = current_cache[0]\n",
    "        A_prev,W,b = linear_cache\n",
    "\n",
    "        dZ = dA*self.deactivate(dA,n_layer)\n",
    "        dW = (1/batch_size)*dZ.dot(A_prev.T) + (self.lamb/batch_size)*self.parameters['W'+str(n_layer)]\n",
    "        db = (1/batch_size)*np.sum(dZ,keepdims=True,axis=1)\n",
    "        dA_prev = W.T.dot(dZ)\n",
    "\n",
    "        assert(dA_prev.shape == A_prev.shape)\n",
    "        assert(dW.shape == W.shape)\n",
    "        assert(db.shape == b.shape)\n",
    "        \n",
    "        return dW,db,dA_prev\n",
    "        \n",
    "        \n",
    "\n",
    "    def backward(self,prediction: ndarray,mappings: ndarray) -> None:\n",
    "        '''\n",
    "        Backward propagates through the network and stores useful calculations\n",
    "        :param prediction: Output of neural net\n",
    "        :param mapping: Correct output of the function\n",
    "        :return : None\n",
    "        '''\n",
    "        layer_num = len(self.cache)\n",
    "        doutput = self.output_backward(prediction,mappings)\n",
    "        self.grads['dW'+str(layer_num)],self.grads['db'+str(layer_num)],self.grads['dA'+str(layer_num-1)] = self.linear_backward(doutput,layer_num)\n",
    "        temp = self.layer_type\n",
    "        self.layer_type = self.layer_type[1:]\n",
    "        \n",
    "        for l in reversed(range(layer_num-1)):\n",
    "            dW,db,dA_prev = None,None,None\n",
    "            if self.layer_type[l] == 'fc':\n",
    "                dW,db,dA_prev = self.linear_backward(self.grads['dA'+str(l+1)],l+1)\n",
    "            elif self.layer_type[l] == 'conv':\n",
    "                dW,db,dA_prev = self.conv_backward((self.cache[l][1][0]),self.cache[l][0])\n",
    "            self.grads['dW'+str(l+1)] = dW\n",
    "            self.grads['db'+str(l+1)] = db\n",
    "            self.grads['dA'+str(l)] = dA_prev\n",
    "        \n",
    "        self.layer_type = temp\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_pad(imgData: ndarray,pad: int) -> ndarray:\n",
    "        '''\n",
    "        Provides zero padding to the multi channel image data provided\n",
    "        :param imgData: image data to pad\n",
    "        :param pad    : amount of padding per layer\n",
    "        :return : image with desired padding\n",
    "        '''\n",
    "        X = np.pad(imgData,((0,0),(pad,pad),(pad,pad),(0,0)),'constant',constant_values = 0)\n",
    "        return X\n",
    "\n",
    "    def conv2d(self,in_planes: int,out_planes: int,kernel_size: int,activation: str,stride: int = 1,padding: int = 0) -> None:\n",
    "        '''\n",
    "        Add paramters for this layer in the parameters list\n",
    "        :return : None\n",
    "        '''\n",
    "        num_layers = int(len(self.parameters)/2)\n",
    "        self.parameters['W'+str(num_layers+1)] = np.random.randn(kernel_size,kernel_size,in_planes,out_planes)\n",
    "        self.parameters['b'+str(num_layers+1)] = np.random.randn(1,1,1,out_planes)\n",
    "        self.activations.append(activation)\n",
    "        self.layer_type.append('conv')\n",
    "        self.hyperparam[num_layers+1] = list((stride,padding))\n",
    "\n",
    "    def conv_single(self,a_prev_slice: ndarray,W: ndarray,b: ndarray) -> float64:\n",
    "        '''\n",
    "        Apply convolution using W and b as filter on the activation slice of the previous layer\n",
    "        :param a_prev_slice: a slice of previous activated layer\n",
    "        :param W           : Filter\n",
    "        :param b           : bais\n",
    "        :return Z: scalar value resultant of the convolution\n",
    "        '''\n",
    "        Z  = np.multiply(a_prev_slice,W)\n",
    "        Z = np.sum(Z)\n",
    "        Z = Z + float(b) #to convert the value to float from matrix type\n",
    "        return Z\n",
    "    \n",
    "    def conv_forward(self,A_prev: ndarray,W: ndarray,b: ndarray,hyper_param: List[int]) -> Tuple[ndarray, Tuple[ndarray, ndarray, ndarray, List[int]]]:\n",
    "        '''\n",
    "        Implements forward pass of convolutional layer.\n",
    "        \n",
    "        :param A_prev:activations of previous layer\n",
    "        :param W: Filter\n",
    "        :param b: bias\n",
    "        :param hyper_param  : list of hyperparameters, stride and padding\n",
    "        :return: Z,cache\n",
    "        '''\n",
    "        m,h_prev,w_prev,nc_prev = A_prev.shape\n",
    "        f,f,nc_prev,nc = W.shape\n",
    "        stride,pad = hyper_param\n",
    "        #comupte the dimensions of the result using convolution formula => w/h = (w/h(prev) -f +2*pad)/stride +1\n",
    "        n_h = int(np.floor((h_prev-f+2*pad)/stride)) +1\n",
    "        n_w = int(np.floor((w_prev-f+2*pad)/stride)) +1\n",
    "        \n",
    "        Z = np.zeros((m,n_h,n_w,nc))\n",
    "        A_prev_pad = self.zero_pad(A_prev,pad)\n",
    "        for i in range(m):\n",
    "            prev_pad = A_prev_pad[i]\n",
    "\n",
    "            for h in range(n_h):\n",
    "                for w in range(n_w):\n",
    "                    for c in range(nc):\n",
    "                        vertstart = h*stride\n",
    "                        vertend = vertstart + f\n",
    "                        horstart = w*stride\n",
    "                        horend = horstart+f \n",
    "\n",
    "                        prev_slice = prev_pad[vertstart:vertend,horstart:horend,:]\n",
    "                        Z[i,h,w,c] = self.conv_single(prev_slice,W[:,:,:,c],b[:,:,:,c])\n",
    "        \n",
    "        cache = (A_prev,W,b,hyper_param)\n",
    "\n",
    "        return Z,cache\n",
    "\n",
    "    def pool_forward(self,A_prev,f,stride,type='max'):\n",
    "        '''\n",
    "        To enable max and average pooling during the forward pass\n",
    "        :param A_prev: Activation of previous layer\n",
    "        :param   f   : filter size\n",
    "        :param stride: size of each stride\n",
    "        :param type  : type of pooling, max or average\n",
    "        \n",
    "        :returns A,cache:\n",
    "        '''\n",
    "        #Calculate the resultant dimensions:\n",
    "        n_h = int(1 + (A_prev.shape[1] - f) / stride)\n",
    "        n_w = int(1 + (A_prev.shape[2] - f) / stride)\n",
    "        n_c = A_prev.shape[3]\n",
    "\n",
    "        A = np.zeros((A_prev.shape[0],n_h,n_w,n_c))\n",
    "\n",
    "        for i in range(A.shape[0]):\n",
    "            for h in range(n_h):\n",
    "                for w in range(n_w):\n",
    "                    for c in range(n_c):\n",
    "                        vertstart = h*stride\n",
    "                        vertend = vertstart + f\n",
    "                        horstart = w*stride\n",
    "                        horend = horstart+f \n",
    "\n",
    "                        a_prev_slice = A_prev[i,vertstart:vertend,horstart:horend,c]\n",
    "\n",
    "                        if type == 'max':\n",
    "                            A[i,h,w,c] = np.max(a_prev_slice)\n",
    "                        elif type == 'avg':\n",
    "                            A[i,h,w,c] = np.mean(a_prev_slice)\n",
    "        \n",
    "        cache = (A_prev,[f,stride],type)\n",
    "        return A,cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def conv_backward(self,dZ: ndarray, cache: Tuple[ndarray, ndarray, ndarray, List[int]]) -> Tuple[ndarray, ndarray, ndarray]:\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, n_C)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        (A_prev, W, b, hparameters) = cache\n",
    "        \n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "        \n",
    "        (f, f, n_C_prev, n_C) = W.shape\n",
    "        \n",
    "        stride = hparameters[0]\n",
    "        pad = hparameters[1]\n",
    "        \n",
    "        (m, n_H, n_W, n_C) = dZ.shape\n",
    "        \n",
    "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "        dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "        db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "        A_prev_pad = self.zero_pad(A_prev, pad)\n",
    "        dA_prev_pad = self.zero_pad(dA_prev, pad)\n",
    "        \n",
    "        for i in range(m):                      \n",
    "            \n",
    "            a_prev_pad = A_prev_pad[i]\n",
    "            da_prev_pad = dA_prev_pad[i]\n",
    "            \n",
    "            for h in range(n_H):                  \n",
    "                for w in range(n_W):               \n",
    "                    for c in range(n_C):           \n",
    "                        \n",
    "                        vert_start = h * stride\n",
    "\n",
    "                        vert_end = vert_start + f\n",
    "                        horiz_start = w * stride\n",
    "\n",
    "                        horiz_end = horiz_start + f\n",
    "                        \n",
    "                        a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                        da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                        dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                        db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                        \n",
    "            dA_prev[i, :, :, :] =  da_prev_pad if pad == 0 else da_prev_pad[pad:-pad,pad:-pad,:]\n",
    "        \n",
    "        assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "    def create_mask(self,X):\n",
    "        '''\n",
    "        Creates mask of from a slice which sets max element index to 1 and others to 0\n",
    "        :param X: original matrix\n",
    "        :return :mask\n",
    "        '''\n",
    "        mask = (X==np.max(X))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def average_back(self,X,shape):\n",
    "        '''\n",
    "        Computes backward pass for average pooling layer\n",
    "        :param X: average pooled layer\n",
    "        :param shape: shape of the original matrix\n",
    "        '''\n",
    "        h,w = shape\n",
    "        X = X/(h*w)\n",
    "        return np.ones(shape)*X\n",
    "\n",
    "    def pool_backward(self,dA, cache, mode = \"max\"):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the pooling layer\n",
    "        \n",
    "        :param dA: gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "        :param cache: cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "        :param mode:the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev  gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "        \"\"\"\n",
    "        \n",
    "        (A_prev, (stride,f),type) = cache\n",
    "        \n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        m, n_H, n_W, n_C = dA.shape\n",
    "        \n",
    "        dA_prev = np.zeros(A_prev.shape)\n",
    "        \n",
    "        for i in range(m):                       \n",
    "            a_prev = A_prev[i]\n",
    "            for h in range(n_H):                   \n",
    "                for w in range(n_W):               \n",
    "                    for c in range(n_C):           \n",
    "                        \n",
    "                        vert_start = h*stride\n",
    "                        vert_end = vert_start + f\n",
    "                        horiz_start = w*stride\n",
    "                        horiz_end = horiz_start + f\n",
    "                        \n",
    "                        \n",
    "                        if type == \"max\":\n",
    "                        \n",
    "                            a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                            mask = self.create_mask(a_prev_slice)\n",
    "                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        elif mode == \"average\":\n",
    "                            da = dA[i, h, w, c]\n",
    "                            shape = (f, f)\n",
    "                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += self.average_back(da, shape)\n",
    "                        \n",
    "    \n",
    "    \n",
    "        return dA_prev                            \n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        '''\n",
    "        :Return: the network architecture and connectivity\n",
    "        '''\n",
    "        net_string = \"\"\n",
    "        for params in range(int(len(self.parameters)/2)):\n",
    "            weight = self.parameters['W'+str(params+1)]\n",
    "            net_string = net_string + \" -> Linear(\" + str(weight.shape[1]) +\" , \" + str(weight.shape[0]) + \")\"\n",
    "            if self.activations[params] != None:\n",
    "                net_string = net_string + \" -> \" +  self.activations[params]\n",
    "        return net_string\n",
    "    \n",
    "     \n",
    "def predict(self,x):\n",
    "        y_out = self.forward(x)\n",
    "        return np.argmax(y_out,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
